import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
from colorama import Fore, Style, init

# Initialize colorama
init(autoreset=True)

def crawl_website(start_url):
    """Crawls a website to discover all unique internal and external links."""
    print(f"\n{Style.BRIGHT}Starting crawl at: {start_url}{Style.RESET_ALL}")
    
    domain_name = urlparse(start_url).netloc
    urls_to_visit = deque([start_url])
    visited_urls = {start_url}
    internal_links = set()
    external_links = set()

    try:
        while urls_to_visit:
            current_url = urls_to_visit.popleft()
            print(f"{Fore.CYAN}Crawling: {current_url}")

            try:
                response = requests.get(current_url, timeout=3)
                soup = BeautifulSoup(response.content, 'html.parser')
            except (requests.RequestException, requests.exceptions.ReadTimeout) as e:
                print(f"{Fore.RED}  -> Could not fetch URL: {e}")
                continue

            for a_tag in soup.find_all('a', href=True):
                href = a_tag['href']
                full_url = urljoin(current_url, href)
                
                if full_url in visited_urls:
                    continue
                
                visited_urls.add(full_url)
                
                if domain_name in urlparse(full_url).netloc:
                    internal_links.add(full_url)
                    urls_to_visit.append(full_url)
                else:
                    external_links.add(full_url)

    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}Crawl stopped by user.")

    print(f"\n{Style.BRIGHT}--- Crawl Report ---")
    print(f"Total pages visited: {len(visited_urls)}")
    print(f"{Fore.GREEN}Found {len(internal_links)} unique internal links.")
    print(f"{Fore.YELLOW}Found {len(external_links)} unique external links.")
    # Optionally, print the links themselves
    # print("\nInternal Links:")
    # for link in sorted(internal_links):
    #     print(link)

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print(f"{Fore.YELLOW}Usage: python {sys.argv[0]} <start_url>")
        sys.exit(1)

    crawl_website(sys.argv[1])
